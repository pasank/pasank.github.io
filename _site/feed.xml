<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://pasank.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://pasank.com/" rel="alternate" type="text/html" /><updated>2017-07-18T19:51:31+10:00</updated><id>http://pasank.com/</id><title type="html">Pasan Karunaratne</title><entry><title type="html">How to clear logs and restart Logstash</title><link href="http://pasank.com/how-to-clear-old-logs-and-restart-logstash/" rel="alternate" type="text/html" title="How to clear logs and restart Logstash" /><published>2017-07-18T19:09:00+10:00</published><updated>2017-07-18T19:09:00+10:00</updated><id>http://pasank.com/how-to-clear-old-logs-and-restart-logstash</id><content type="html" xml:base="http://pasank.com/how-to-clear-old-logs-and-restart-logstash/">&lt;p&gt;I run an &lt;a href=&quot;http://storm.apache.org&quot;&gt;Apache Storm&lt;/a&gt; cluster and use the &lt;a href=&quot;https://www.elastic.co/webinars/introduction-elk-stack&quot;&gt;ELK Stack&lt;/a&gt; to preserve my sanity when analysing logs from different machines on the cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pasank.com/assets/images/logstash-clear-restart-post/kibana-capture-1.png&quot; alt=&quot;ELK Stack on Apache Storm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s pretty cool and has saved me from pulling my hair out grepping and tailing log files from a number of machines.&lt;/p&gt;

&lt;p&gt;However, when ingesting data at very high rates Logstash sometimes fails and stops listening on port 5044. Doing a simple restart just results in Logstash trying to process the old logs that it is yet to index, which slows it down to a crawl yet again.&lt;/p&gt;

&lt;p&gt;The procedure to clear out the logs from Logstash and start afresh strangely seems to not be specified in the documentation. Hence this post.&lt;/p&gt;

&lt;p&gt;Logstash stores the logs it reads in the folder &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/log/logstash&lt;/code&gt; (on Ubuntu 16.04) in the form of &lt;code class=&quot;highlighter-rouge&quot;&gt;logstash.stdout.*&lt;/code&gt; files.&lt;/p&gt;

&lt;p&gt;Delete the &lt;code class=&quot;highlighter-rouge&quot;&gt;logstash.stdout.*&lt;/code&gt; files in this location, and then restart logstash using&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl restart logstash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Do a &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo systemctl status logstash&lt;/code&gt; for good measure.&lt;/p&gt;

&lt;hr /&gt;</content><author><name>pasankarunaratne</name></author><category term="blog" /><category term="sysadmin" /><category term="elk" /><category term="tidbits" /><category term="Storm" /><summary type="html">I run an Apache Storm cluster and use the ELK Stack to preserve my sanity when analysing logs from different machines on the cluster.</summary></entry><entry><title type="html">What is a hyperparameter?</title><link href="http://pasank.com/what-is-a-hyperparameter/" rel="alternate" type="text/html" title="What is a hyperparameter?" /><published>2017-06-28T20:26:00+10:00</published><updated>2017-06-28T20:26:00+10:00</updated><id>http://pasank.com/what-is-a-hyperparameter</id><content type="html" xml:base="http://pasank.com/what-is-a-hyperparameter/">&lt;p&gt;A hyperparameter is a knob.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pasank.com/assets/images/hyperparameter-post/knob.jpg&quot; alt=&quot;knob&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a knob that you tweak in your model to control its learning process.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hyper&lt;/em&gt;parameters are a bit different from what we normally call a parameter.&lt;/p&gt;

&lt;p&gt;Parameters serve as a representation of your data. A &lt;em&gt;parameteric&lt;/em&gt; type of model represents your data as a set of parameters.&lt;/p&gt;

&lt;p&gt;For example, the data given below can be represented with a linear model with the parameters (m = 3, c = 7)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pasank.com/assets/images/hyperparameter-post/linear-model.png&quot; alt=&quot;linear-model-parameters&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to model parameters, hyperparameters do not represent data. Hyperparameters are not part of the model that describes the data. Hyperparameters are &lt;em&gt;generally&lt;/em&gt; not learned and not tuned during the learning process.&lt;/p&gt;

&lt;p&gt;Hyperparameters are the control knobs to the learning process. Some examples of hyperparameters are the learning rate (in gradient descent) or the number of leaves (in a decision tree).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Fine print&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A more rigorous explanation would be that a hyperparameter encodes prior belief about how your parameters are distributed.&lt;/p&gt;

&lt;p&gt;You pick a distribution that you think will describe your data. This distribution would have parameters (for example, the normal distribution has the parameters mean and standard deviation). If you pick a distribution that describes those parameters, the parameters of this ‘higher-level’ distribution would be the hyperparameters.&lt;/p&gt;

&lt;p&gt;In short, hyperparameters are the parameters of a distribution that is put on the model parameters.&lt;/p&gt;

&lt;hr /&gt;</content><author><name>pasankarunaratne</name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">A hyperparameter is a knob.</summary></entry></feed>